# -*- coding: utf-8 -*-
"""PhasePrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JZaNgqrQJ0IGCop2lNw6AxdoSedr3ZCu
"""

!pip install --upgrade imbalanced-learn
!pip install --upgrade xgboost==2.1.4

import pandas as pd
import numpy as np
import re
from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
from imblearn.combine import SMOTETomek  # This should now work
from xgboost import XGBClassifier
import shap
import matplotlib.pyplot as plt
import seaborn as sns

import sklearn
print("scikit-learn version:", sklearn.__version__)  # Should be 1.3.2
import imblearn
print("imbalanced-learn version:", imblearn.__version__)  # Should be 0.12.0
import numpy
print("numpy version:", numpy.__version__)  # Should be 1.26.4

# =========================
# 1. Data Loading & Cleaning
# =========================
def load_and_clean_data(path):
    df = pd.read_csv("/content/HEA Phase DataSet v1d.csv", encoding='cp1252')
    df = df.rename(columns={'Alloy ': 'Alloy', 'Sythesis_Route': 'Synthesis_Route'})
    df = df.drop(columns=['Unnamed: 51', 'Unnamed: 52', 'HPR'], errors='ignore')
    # Drop rows with missing essential data
    df = df.dropna(subset=['Alloy', 'Phases', 'dHmix', 'dSmix'])
    df['Alloy'] = df['Alloy'].astype(str)
    return df

# =========================
# 2. Feature Engineering
# =========================
atomic_radii = {
    'Al':1.43, 'Co':1.25, 'Cr':1.28, 'Fe':1.26, 'Ni':1.24,
    'Cu':1.28, 'Mn':1.26, 'Ti':1.47, 'V':1.34, 'Nb':1.43,
    'Mo':1.39, 'Zr':1.60, 'Hf':1.59, 'Ta':1.43, 'W':1.41
}
vec_values = {
    'Al':3, 'Nb':5, 'Ta':5, 'Ti':4, 'V':5, 'Mo':6, 'W':6,
    'Cr':6, 'Fe':8, 'Ni':10, 'Co':9, 'Cu':11, 'Zr':4, 'Hf':4
}

def parse_alloy(alloy_str):
    if not isinstance(alloy_str, str) or pd.isna(alloy_str):
        return []
    elements = re.findall(r'[A-Z][a-z]*\d*\.?\d*', alloy_str)
    return [re.sub(r'\d+\.?\d*', '', e) for e in elements]

def calculate_features(row):
    elements = parse_alloy(row['Alloy'])
    compositions = [row[e] for e in elements if e in row.index]
    radii = [atomic_radii[e] for e in elements if e in atomic_radii]
    avg_radius = np.mean(radii) if radii else np.nan
    delta = np.sqrt(sum(c*(1-c)*(r-avg_radius)**2 for c,r in zip(compositions, radii))) if compositions else np.nan
    mixing_entropy = -sum(c*np.log(c) for c in compositions if c > 0) if compositions else np.nan
    true_vec = sum(row[e]*vec_values.get(e,0) for e in elements if e in row.index)
    stability_param = row['dHmix'] / (row['Tm'] + 1e-6) if 'Tm' in row and not pd.isna(row['Tm']) else np.nan
    return pd.Series({
        'Avg_Atomic_Radius': avg_radius,
        'Delta': delta,
        'Mixing_Entropy': mixing_entropy,
        'True_VEC': true_vec,
        'Stability_Param': stability_param,
        'Num_Elements': len(elements)
    })

def prepare_data(df):
    feature_df = df.apply(calculate_features, axis=1)
    df = pd.concat([df, feature_df], axis=1)
    features = [
        'dHmix', 'dSmix', 'VEC', 'Atom.Size.Diff',
        'Avg_Atomic_Radius', 'Delta', 'Mixing_Entropy',
        'True_VEC', 'Stability_Param', 'Num_Elements'
    ]
    # Drop rows with missing features
    df = df.dropna(subset=features)
    X = df[features]
    y = df['Phases']
    return X, y

# =========================
# 3. Main Workflow
# =========================
from sklearn.utils.class_weight import compute_class_weight

if __name__ == "__main__":
    # 1. Load Data
    df = load_and_clean_data("/kaggle/input/datasethea/HEA Phase DataSet v1d.csv")

    # 2. Feature Engineering
    X, y = prepare_data(df)

    # 3. Encode Target Labels
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)

    # 4. Compute Class Weights
    classes = np.unique(y_encoded)
    class_weights = compute_class_weight('balanced', classes=classes, y=y_encoded)
    sample_weights = np.array([class_weights[y] for y in y_encoded])

    # 5. Train-Test Split & Scaling
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_encoded, test_size=0.2,
        random_state=42, stratify=y_encoded
    )
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # 6. Handle Class Imbalance
    smt = SMOTETomek(random_state=42)
    X_bal, y_bal = smt.fit_resample(X_train_scaled, y_train)
    print("\nBalanced Class Distribution (encoded):\n", pd.Series(y_bal).value_counts())

    # 7. Model Training with Class Weights
    params = {
        'learning_rate': [0.01, 0.05, 0.1, 0.2],
        'max_depth': [3, 5, 7, 9],
        'subsample': [0.6, 0.8, 1.0],
        'colsample_bytree': [0.6, 0.8, 1.0],
        'gamma': [0, 0.1, 0.2],
        'reg_lambda': [0, 0.1, 0.5],
        'reg_alpha': [0, 0.1, 0.5]
    }

    model = XGBClassifier(
        tree_method='hist',
        n_estimators=500,
        objective='multi:softprob',
        eval_metric='logloss',
        random_state=42
    )

    search = RandomizedSearchCV(
        model, params,
        n_iter=50,
        scoring='f1_weighted',
        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
        random_state=42,
        n_jobs=-1
    )

    # Apply sample weights to the balanced data
    bal_sample_weights = np.array([class_weights[y] for y in y_bal])
    search.fit(X_bal, y_bal, sample_weight=bal_sample_weights)

    best_model = search.best_estimator_
    print(f"\nBest Parameters: {search.best_params_}")

    # 8. Evaluation
    y_pred = best_model.predict(X_test_scaled)
    print("\nClassification Report:")
    print(classification_report(
        le.inverse_transform(y_test),
        le.inverse_transform(y_pred),
        target_names=le.classes_
    ))

    # 9. Enhanced Confusion Matrix
    plt.figure(figsize=(10,8))
    cm = confusion_matrix(le.inverse_transform(y_test), le.inverse_transform(y_pred),
                         labels=le.classes_)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=le.classes_, yticklabels=le.classes_,
                cbar_kws={'label': 'Number of Alloys'})
    plt.xlabel('Predicted Phase', fontsize=12)
    plt.ylabel('True Phase', fontsize=12)
    plt.title('Phase Prediction Confusion Matrix', fontsize=14)
    plt.xticks(rotation=45)
    plt.yticks(rotation=0)
    plt.show()

    # 10. SHAP Analysis with Class Names
    explainer = shap.TreeExplainer(best_model)
    shap_values = explainer.shap_values(X_test_scaled)

    plt.figure(figsize=(12,8))
    shap.summary_plot(shap_values, X_test_scaled,
                     feature_names=X.columns,
                     class_names=le.classes_,
                     plot_type='bar')
    plt.title("Phase Prediction Feature Importance", fontsize=14)
    plt.tight_layout()
    plt.show()

import pandas as pd
misclassified = X_test.copy()
misclassified['True_Phase'] = le.inverse_transform(y_test)
misclassified['Predicted_Phase'] = le.inverse_transform(y_pred)
misclassified = misclassified[misclassified['True_Phase'] != misclassified['Predicted_Phase']]
print(misclassified[['True_Phase', 'Predicted_Phase']].value_counts())

plt.figure(figsize=(10,6))
sns.boxplot(data=df, x='Phases', y='VEC', palette='Set3')
plt.title("Distribution of VEC by Phase")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(8,5))
sns.countplot(x='Phases', data=df, order=df['Phases'].value_counts().index, palette='Set2')
plt.title("Class Distribution of Alloy Phases")
plt.xlabel("Phase")
plt.ylabel("Count")
plt.tight_layout()
plt.show()

importances = best_model.feature_importances_
indices = np.argsort(importances)[::-1]
feature_names = X.columns

plt.figure(figsize=(10,6))
plt.title("Feature Importances (RandomForest)")
plt.bar(range(len(importances)), importances[indices], align="center")
plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=45)
plt.tight_layout()
plt.show()

import joblib

# 1. Save the scaler (crucial for new data preprocessing)
joblib.dump(scaler, 'hea_scaler.pkl')

# 2. Save the label encoder (to decode phase names)
joblib.dump(le, 'label_encoder.pkl')

# 3. Save the trained model
joblib.dump(best_model, 'hea_phase_predictor.pkl')

def predict_phase(new_data):
    # Load components
    scaler = joblib.load('hea_scaler.pkl')
    le = joblib.load('label_encoder.pkl')
    model = joblib.load('hea_phase_predictor.pkl')

    # Preprocess new data
    scaled_data = scaler.transform(new_data)

    # Predict
    pred_encoded = model.predict(scaled_data)

    # Convert numbers to phase names
    return le.inverse_transform(pred_encoded)

# Sample new alloy data (must match your feature columns!)
new_alloy = pd.DataFrame([[ -12.5, 13.2, 4.8, 5.1, 1.42, 0.04, 1.58, 5.3, -0.01, 5 ]],
                         columns=X.columns)  # Replace X.columns with your actual feature names
print(predict_phase(new_alloy))  # Output: ['BCC_SS']

# Test with known data from your test set
test_sample = X_test.iloc[[0]]  # First test sample
print("True Phase:", le.inverse_transform(y_test)[0])
print("Predicted:", predict_phase(test_sample)[0])

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import warnings
# warnings.filterwarnings("ignore", message="missing ScriptRunContext", category=UserWarning)
# 
# import streamlit as st
# import pandas as pd
# import numpy as np
# import joblib
# 
# 
# 
# 
# # Load model and preprocessing objects
# @st.cache_resource
# def load_model():
#     model = joblib.load("hea_phase_predictor.pkl")
#     scaler = joblib.load("hea_scaler.pkl")
#     le = joblib.load("label_encoder.pkl")
#     return model, scaler, le
# 
# model, scaler, le = load_model()
# 
# # Feature information dictionary
# feature_info = {
#     'dHmix': {'desc': 'Mixing enthalpy (kJ/mol). Typical range: -20 to 5', 'range': (-20, 5)},
#     'dSmix': {'desc': 'Mixing entropy (J/mol·K). Typical range: 10 to 16', 'range': (10, 16)},
#     'VEC': {'desc': 'Valence Electron Concentration. Typical range: 4.5 to 8.5', 'range': (4.5, 8.5)},
#     'Atom.Size.Diff': {'desc': 'Atomic size difference (%). Typical range: 0 to 7', 'range': (0, 7)},
#     'Avg_Atomic_Radius': {'desc': 'Average atomic radius (Å). Typical range: 1.2 to 1.6', 'range': (1.2, 1.6)},
#     'Delta': {'desc': 'Atomic size mismatch (dimensionless). Typical range: 0.01 to 0.08', 'range': (0.01, 0.08)},
#     'Mixing_Entropy': {'desc': 'Configurational entropy (J/mol·K). Typical range: 1.3 to 1.7', 'range': (1.3, 1.7)},
#     'True_VEC': {'desc': 'Calculated VEC (dimensionless). Typical range: 4.5 to 8.5', 'range': (4.5, 8.5)},
#     'Stability_Param': {'desc': 'Stability parameter (dHmix/Tm). Typical range: -0.02 to 0.02', 'range': (-0.02, 0.02)},
#     'Num_Elements': {'desc': 'Number of elements in alloy. Typical range: 4 to 7', 'range': (4, 7)}
# }
# 
# st.title("HEA Phase Predictor")
# st.markdown("Enter your alloy features below. All values should be numerical.")
# 
# with st.form("input_form"):
#     st.caption("ℹ️ Hover over input fields to see feature descriptions and valid ranges.")
# 
#     dHmix = st.number_input("Mixing enthalpy (dHmix in kJ/mol)", value=0.0,
#                             help=feature_info['dHmix']['desc'])
#     dSmix = st.number_input("Mixing entropy (dSmix in J/mol·K)", value=12.0,
#                             help=feature_info['dSmix']['desc'])
#     VEC = st.number_input("Valence Electron Concentration (VEC)", value=7.0,
#                           help=feature_info['VEC']['desc'])
#     Atom_Size_Diff = st.number_input("Atomic size difference (%)", value=5.0,
#                                      help=feature_info['Atom.Size.Diff']['desc'])
#     Avg_Atomic_Radius = st.number_input("Average atomic radius (Å)", value=1.3,
#                                         help=feature_info['Avg_Atomic_Radius']['desc'])
#     Delta = st.number_input("Atomic size mismatch (Δ)", value=0.05,
#                             help=feature_info['Delta']['desc'])
#     Mixing_Entropy = st.number_input("Mixing entropy (S_mix in J/K·mol)", value=1.5,
#                                      help=feature_info['Mixing_Entropy']['desc'])
#     True_VEC = st.number_input("True VEC", value=7.0,
#                                help=feature_info['True_VEC']['desc'])
#     Stability_Param = st.number_input("Stability parameter (dHmix/Tm)", value=-0.003,
#                                       help=feature_info['Stability_Param']['desc'])
#     Num_Elements = st.select_slider("Number of elements in alloy", options=[4, 5, 6, 7],
#                                     value=5, help=feature_info['Num_Elements']['desc'])
# 
#     submit = st.form_submit_button("Predict Phase")
# 
# if submit:
#     # Input validation
#     input_dict = {
#         'dHmix': dHmix,
#         'dSmix': dSmix,
#         'VEC': VEC,
#         'Atom.Size.Diff': Atom_Size_Diff,
#         'Avg_Atomic_Radius': Avg_Atomic_Radius,
#         'Delta': Delta,
#         'Mixing_Entropy': Mixing_Entropy,
#         'True_VEC': True_VEC,
#         'Stability_Param': Stability_Param,
#         'Num_Elements': Num_Elements
#     }
#     valid = True
#     for feat, value in input_dict.items():
#         min_val, max_val = feature_info[feat]['range']
#         if not (min_val <= value <= max_val):
#             st.error(f"❗ {feat} must be between {min_val} and {max_val}. You entered {value}.")
#             valid = False
# 
#     if valid:
#         features = pd.DataFrame([input_dict.values()], columns=input_dict.keys())
#         try:
#             scaled = scaler.transform(features)
#             pred = model.predict(scaled)
#             phase = le.inverse_transform(pred)[0]
#             proba = model.predict_proba(scaled)[0]
#             confidence = np.max(proba) * 100
# 
#             # Top 3 predictions
#             top3_idx = np.argsort(proba)[::-1][:3]
#             st.success(f"Predicted Phase: **{phase}** (Confidence: {confidence:.1f}%)")
#             st.markdown("#### Top 3 Phase Probabilities:")
#             for idx in top3_idx:
#                 st.write(f"- {le.inverse_transform([idx])[0]}: {proba[idx]*100:.1f}%")
#         except Exception as e:
#             st.error(f"Prediction failed: {str(e)}")
#

from google.colab import files
uploaded = files.upload()

!pip install pyngrok streamlit --quiet

from pyngrok import ngrok
import os
import time

# Set your authtoken
ngrok.set_auth_token("2xOQYyy5CDxhaVZZUD5X4mDCzoT_33Nnk9ZaGEeWc7nXpsrqe")

# Start Streamlit in the background
os.system('nohup streamlit run app.py --server.port 8501 &')

# Wait for server to start
time.sleep(5)

# Open the tunnel with explicit protocol
public_url = ngrok.connect(8501, "http")
print(f"Streamlit app is live at: {public_url}")

